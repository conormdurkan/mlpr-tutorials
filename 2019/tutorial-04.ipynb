{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import special, stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference\n",
    "\n",
    "For data $ \\mathcal{D} $ and parameters $ \\boldsymbol{\\theta} $, Bayes' rule tells us\n",
    "$$ p(\\boldsymbol{\\theta} \\vert \\mathcal{D}) = \\frac{p(\\mathcal{D} \\vert \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta})}{p(\\mathcal{D})} \\propto p(\\mathcal{D} \\vert \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}). $$\n",
    "\n",
    "- $ p\\boldsymbol{\\theta} \\vert \\mathcal{D}) $ is the **posterior** distribution over $ \\boldsymbol{\\theta} $ given $ \\mathcal{D} $; it characterizes our updated beliefs about $ \\boldsymbol{\\theta} $ after we've observed data $ \\mathcal{D} $.\n",
    "- $ p(\\mathcal{D} \\vert \\boldsymbol{\\theta}) $ is the conditional distribution over $ \\mathcal{D} $ given $ \\boldsymbol{\\theta} $, or alternatively, the **likelihood** of $ \\boldsymbol{\\theta} $ given data $ \\mathcal{D} $ i.e. how well the parameters $ \\boldsymbol{\\theta} $ explain the data. The likelihood is a function of $ \\boldsymbol{\\theta} $, and from this point of view, **the likelihood function is not a probability distribution**. Go back and read this point again.\n",
    "- $ p(\\boldsymbol{\\theta}) $ is the **prior** distribution over $ \\boldsymbol{\\theta} $; it characterizes our beliefs about the parameters before we've made any observations or gathered any data.\n",
    "- $ p(\\mathcal{D}) = \\int p(\\mathcal{D}, \\boldsymbol{\\theta}) \\ \\mathrm{d} \\boldsymbol{\\theta} = \\int p(\\mathcal{D} \\vert \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta}) \\ \\mathrm{d} \\boldsymbol{\\theta} $ is the **evidence** or **marginal likelihood** of the data, or the average value of the likelihood under the prior; it characterizes the probability of seeing data $ \\mathcal{D} $ when we use a particular likelihood and prior i.e. when we use a particular model for the data. For this reason, the marginal likelihood is often used for model comparison.\n",
    "\n",
    "Bayesian inference refers to the broad challenge of computing the posterior distribution for an arbitrary likelihood function and prior distribution. Generally, the product of the likelihood and prior gives rise to a complex, intractable distribution which is difficult to reason about and work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate priors\n",
    "The idea of conjugacy in Bayesian inference is an appealing one. We say a particular prior is conjugate to some likelihood if the resulting posterior distribution is in the same family as the prior. Recalling that the posterior is generally a complex, unwieldy distribution for an arbitrary choice of likelihood and prior, conjugacy is indeed a very desirable property, since we can compute the posterior in closed form. \n",
    "\n",
    "## An example of a conjugate prior\n",
    "\n",
    "Imagine someone approaches you and asks whether the coin they're holding is fair. Your accoster, a wise Bayesian, is not interested in a binary decision about its fairness, but wants you to quantify your beliefs over the entire range of its possible fairness. You are allowed to flip the coin as often as you like for the next few minutes.\n",
    "\n",
    "For a moment, we consider just flipping the coin as fast as we can, keeping a running average of the number of heads, and returning that answer when we're out of time. However, if something goes wrong, and the stranger is particularly mischievous, we may only get a few flips before we're forced to stop, and our guess will be heavily influenced by the first few outcomes. Even a fair coin won't justify its fairness in a few flips! With this in mind, remembering the mysterious stranger has no interest in a point estimate, and knowing the value of being Bayesian when expressing our beliefs about events, we decide to tackle this problem with Bayes' rule. This means we need to decide on two things:\n",
    "\n",
    "- What is our prior? \n",
    "- What is our likelihood function? \n",
    "\n",
    "#### The prior distribution\n",
    "There's a natural way to quantify fairness here: our beliefs about the fairness of a coin lie on a continuous scale from 0 to 1. Let's call this random variable, the probability of heads, $ \\theta $. We also know that coins tend to be fair. We're a bit suspicious in this case though, because this isn't an arbitrary coin, it's a challenge from the mysterious stranger. So what properties should our prior have? Well, it should\n",
    "\n",
    "- be a probability distribution over $ [0, 1] $,\n",
    "- be symmetric about $ 0.5 $, since we don't know which way the coin might be biased,\n",
    "- have high density near $ 0.5 $, since we know most coins are fair,\n",
    "- have some moderate variance to account for the fact that we're suspicious of this coin-toting stranger.\n",
    "\n",
    "Fortunately, there exists just such a distribution.\n",
    "\n",
    "The **beta distribution** is a family of continuous probability distributions defined on the interval $ [0, 1] $. It's got two postive parameters $ \\alpha $ and $ \\beta $, and its density function is given by \n",
    "$$ p_{\\alpha, \\beta}(\\theta) = \\frac{\\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}}{B(\\alpha, \\beta)}, \\hspace{0.1cm} \\text{where } B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} $$\n",
    "(we don't need to worry so much about $ B(\\alpha, \\beta) $ here, it's just a normalizing constant). \n",
    "\n",
    "The mean, mode, and variance of the beta distribution are given by $ \\frac{\\alpha}{\\alpha + \\beta} $, $ \\frac{\\alpha - 1}{\\alpha + \\beta - 2} $, and $ \\frac{\\alpha \\beta}{(\\alpha + \\beta)^{2} (\\alpha + \\beta + 1)} $, respectively. We'd like to choose $ \\alpha $ and $ \\beta $ such that the distribution is centred and peaked at $ 0.5 $ (i.e. mean and mode at $ 0.5 $), and its variance is moderate. If we set $ \\alpha = \\beta $, the mean and the mode both lie at $ 0.5 $, and we can use a particular value of the variance to derive a corresponding value of $ \\alpha $ (in the following I've used $ \\alpha = \\beta = 5 $). Let's plot this prior density and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha = beta = 5\n",
    "prior = stats.beta(alpha, beta)\n",
    "theta = np.linspace(0, 1, 1000)\n",
    "ptheta = prior.pdf(theta)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.plot(theta, ptheta)\n",
    "ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n",
    "ax.set_xlabel(r'$ \\theta $')\n",
    "ax.set_ylabel(r'$ p(\\theta) $')\n",
    "ax.fill_between(theta, np.zeros_like(theta), ptheta, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot visualizes our beliefs about the fairness of the coin before we've flipped it. Its characteristics and shape are determined by the pre-existing knowledge we have about the problem before we've gathered any data. That is, the plot visualizes our prior distribution.\n",
    "\n",
    "#### The likelihood function\n",
    "The distribution over a sequence of coin flips also has a natural characterization in the form of the **binomial distribution**. For $ n $ flips, with probability of heads $ \\theta $ on each flip, the probability mass function corresponding to $ k $ heads is \n",
    "$$ p(k \\vert n, \\theta) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} \\theta^{k} (1 - \\theta)^{n - k}. \\tag{1} $$ \n",
    "That is, $ k $ flips must result in heads, the other $ n - k $ flips must result in tails, and the ordering doesn't matter. This is the probability mass function for a binomial distribution over the number of heads given a number of flips $ n $ and a probability of success $ x $. However, fixing some observed sequence of coin flips containing some proportion of heads, it is also the likelihood function for the unknown probability of heads $ x $. That is, \n",
    "$$ L(\\theta) = \\begin{pmatrix} n \\\\ k \\end{pmatrix} \\theta^{k} (1 - \\theta)^{n - k} \\tag{2} $$\n",
    "is the likelihood of the unknown parameter $ \\theta $, given a sequence of $ n $ flips with $ k $ heads. **This is the same expression as above, interpreted differently**. (1) is a probability mass function over the number of heads in a sequence of $ n $ coin flips with probability of heads $ \\theta $, while (2) is the likelihood function (in this case a polynomial) for the unknown parameter $ \\theta $, given that we've observed a particular sequence of flips. Let's plot the likelihood function for different values of the number of heads $ k $, assuming the number of total flips is $ n = 10 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985545755fbe4937afab3c6a85ee1136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='k', max=9, min=1), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def binomial_likelihood(theta, n, k):\n",
    "    return special.comb(n, k) * theta**k * (1 - theta)**(n - k)\n",
    "\n",
    "def plot_likelihood(k=5):\n",
    "    theta = np.linspace(0, 1, 1000)\n",
    "    n = 10\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    likelihood = binomial_likelihood(theta, n, k)\n",
    "    ax.plot(theta, likelihood, color='C2')\n",
    "    ax.set_xlabel(r'$ \\theta $')\n",
    "    ax.set_ylabel(r'$ L(\\theta) $')\n",
    "    ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    ax.set_ylim([0, 0.5])\n",
    "\n",
    "interact(plot_likelihood, k=(1, 9, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 9 different observed data sets (the cases where we've observed 1-9 heads in 10 coin flips), we've plotted the likelihood as a function of $ \\theta $. **Look at where the likelihood function is peaked for each data set**: for data with low numbers of heads, it's peaked near $ 0 $, while for data with high numbers of heads, it's peaked near $ 1 $. Taking the maximum of the likelihood for each of these cases is the (big surprise) maximum likelihood estimate for the probability of heads $ \\theta $, and corresponds to the result you would get if you went along with the original idea of keeping a running tally of the number of heads as you flipped.\n",
    "\n",
    "However, we're Bayesians, and that just isn't good enough. Our posterior is given by multiplying the likelihood by the prior, and renormalizing the result. You should be beginning to see what the result of this multiplication will look like, but lets plot the unnormalized posterior for each of the data sets above to make it clear.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600385c595134719bc3dbc4c0d89225a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='k', max=9, min=1), Output()), _dom_classes=('widget-inte…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_unnormalized_posterior(k=5):\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    alpha = beta = 5\n",
    "    prior = stats.beta(alpha, beta)\n",
    "    \n",
    "    n = 10\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    likelihood = binomial_likelihood(x, n, k)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(x, prior.pdf(x), label='Prior')\n",
    "    ax.fill_between(x, np.zeros_like(x), prior.pdf(x), alpha=0.5)\n",
    "    \n",
    "    ax.plot(x, likelihood, color='C2', label='Likelihood')\n",
    "    \n",
    "    ax.plot(x, likelihood * prior.pdf(x), color='C1', label='Unnormalized posterior')\n",
    "    ax.fill_between(x, np.zeros_like(x), likelihood * prior.pdf(x), alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    ax.legend()\n",
    "\n",
    "interact(plot_unnormalized_posterior, k=(1, 9, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our toy dataset of 10 coin flips, the number of heads $ k $ we observed will change our posterior beliefs about the coin's fairness. This is exactly what's shown above: as we observe different numbers of heads, our unnormalized posterior concentrates on different values of the probability of heads $ \\theta $. \n",
    "\n",
    "Now comes the special bit: it turns out the beta prior is conjugate to the binomial likelihood, and our unnormalized posterior is proportional to another beta distribution! This is easy to see, since our prior is a polynomial in $ \\theta $, and our likelihood is a polynomial in $ \\theta $, so our posterior is a polynomial in $ \\theta $ as well. More concretely, the posterior is \n",
    "\\begin{align}\n",
    "p(\\theta \\vert n, k) &\\propto L(\\theta) p(\\theta) \\\\\n",
    "&\\propto \\theta^{k} (1 - \\theta)^{n - k} \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\\\\n",
    "&= \\theta^{(\\alpha + k) - 1} (1 - \\theta)^{(\\beta + n - k) - 1}.\n",
    "\\end{align}\n",
    "This expression uniquely characterises a beta distribution with parameters $ \\alpha^{\\prime} = \\alpha + k $ and $ \\beta^{\\prime} = \\beta + n - k $. We also know the normalizing constant: it's $ B(\\alpha + k, \\beta + n - k) $ necessarily. In this way, the parameters of the posterior beta distribution can be interpreted as 'counts' of heads and tails, respectively, and computing the posterior just requires addition. \n",
    "\n",
    "Thanks to conjugacy, we now have the exact posterior, so let's plot it, and play with some slightly bigger datasets as well (careful to ensure $ n $ is always bigger than $ k $ in the demo, since we can't have more heads than flips). Notice how as the size of the dataset increases, our posterior becomes increasingly peaked i.e. we become more certain about our beliefs the more data we see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94888af055e4dd68d27d965a8f0f7a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=10, description='n', min=10, step=10), IntSlider(value=5, description='k…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_exact_posterior(n=10, k=5):\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    \n",
    "    alpha = beta = 5\n",
    "    prior = stats.beta(alpha, beta)\n",
    "    \n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    likelihood = binomial_likelihood(x, n, k)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(x, prior.pdf(x), label='Prior')\n",
    "    ax.fill_between(x, np.zeros_like(x), prior.pdf(x), alpha=0.5)\n",
    "    \n",
    "    ax.plot(x, likelihood, color='C2', label='Likelihood')\n",
    "    \n",
    "    alpha_prime = alpha + k\n",
    "    beta_prime = beta + n - k\n",
    "    posterior = stats.beta(alpha_prime, beta_prime)\n",
    "    ax.plot(x, posterior.pdf(x), color='C1', label='Posterior')\n",
    "    ax.fill_between(x, np.zeros_like(x), posterior.pdf(x), alpha=0.5)\n",
    "    \n",
    "    ax.set_xticks([0, 0.25, 0.5, 0.75, 1])\n",
    "    ax.legend()\n",
    "\n",
    "interact(plot_exact_posterior, n=(10, 100, 10), k=(1, 99, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Bayesian stranger is delighted with us; we've not just characterized our beliefs about the coin's fairness having seen data, but we've given him a closed form posterior defined by just two scalar values, $ \\alpha^{\\prime} $ and $ \\beta^{\\prime} $. With just these two numbers, we've fully quantified our beliefs about the coin's fairness, and no matter how much data we ever gather, all we need are two numbers to communicate those beliefs i.e. $ n $ and $ k $ are **sufficient statistics** of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first generate a vector from a standard normal distribution. \n",
    "- Then, for each pass through the loop, we compute the logsumexp of the vector naively, and in a numerically stable way.\n",
    "- We also multiply the vector by 10 in each pass, and show the naive method begins to fail at some point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector is [  9.43510481  -2.10324066 -12.15235771]\n",
      "Naive calculation is: 9.435114560101395\n",
      "Stable calculation is: 9.435114560101395\n",
      "\n",
      "Vector is [  94.35104811  -21.03240664 -121.5235771 ]\n",
      "Naive calculation is: 94.35104810723945\n",
      "Stable calculation is: 94.35104810723945\n",
      "\n",
      "Vector is [  943.51048107  -210.32406638 -1215.23577097]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 943.5104810723944\n",
      "\n",
      "Vector is [  9435.10481072  -2103.2406638  -12152.35770974]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 9435.104810723944\n",
      "\n",
      "Vector is [  94351.04810724  -21032.40663804 -121523.57709742]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 94351.04810723945\n",
      "\n",
      "Vector is [  943510.48107239  -210324.06638038 -1215235.77097416]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 943510.4810723944\n",
      "\n",
      "Vector is [  9435104.81072394  -2103240.66380377 -12152357.70974157]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 9435104.810723944\n",
      "\n",
      "Vector is [ 9.43510481e+07 -2.10324066e+07 -1.21523577e+08]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 94351048.10723944\n",
      "\n",
      "Vector is [ 9.43510481e+08 -2.10324066e+08 -1.21523577e+09]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 943510481.0723944\n",
      "\n",
      "Vector is [ 9.43510481e+09 -2.10324066e+09 -1.21523577e+10]\n",
      "Naive calculation is: inf\n",
      "Stable calculation is: 9435104810.723944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def logsumexp(x):\n",
    "    x_max = max(x)\n",
    "    return x_max + np.log(np.sum(np.exp(x - x_max)))\n",
    "\n",
    "\n",
    "x = np.random.randn(3)\n",
    "for i in range(10):\n",
    "    x *= 10\n",
    "    print('Vector is {:}'.format(x))\n",
    "    naive = np.log(np.sum(np.exp(x)))\n",
    "    print('Naive calculation is: {:}'.format(naive))\n",
    "    stable = logsumexp(x)\n",
    "    print('Stable calculation is: {:}'.format(stable))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
